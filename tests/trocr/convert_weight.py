import torch
import paddle
from safetensors.torch import load_file
loaded_state_dict = load_file("./Exp8/latest/model.safetensors")

convert_list = [
    "encoder.encoder.layer.0.attention.attention.query",
"encoder.encoder.layer.0.attention.attention.key",
"encoder.encoder.layer.0.attention.attention.value",
"encoder.encoder.layer.0.attention.output.dense",
"encoder.encoder.layer.0.intermediate.dense",
"encoder.encoder.layer.0.output.dense",
"encoder.encoder.layer.1.attention.attention.query",
"encoder.encoder.layer.1.attention.attention.key",
"encoder.encoder.layer.1.attention.attention.value",
"encoder.encoder.layer.1.attention.output.dense",
"encoder.encoder.layer.1.intermediate.dense",
"encoder.encoder.layer.1.output.dense",
"encoder.encoder.layer.2.attention.attention.query",
"encoder.encoder.layer.2.attention.attention.key",
"encoder.encoder.layer.2.attention.attention.value",
"encoder.encoder.layer.2.attention.output.dense",
"encoder.encoder.layer.2.intermediate.dense",
"encoder.encoder.layer.2.output.dense",
"encoder.encoder.layer.3.attention.attention.query",
"encoder.encoder.layer.3.attention.attention.key",
"encoder.encoder.layer.3.attention.attention.value",
"encoder.encoder.layer.3.attention.output.dense",
"encoder.encoder.layer.3.intermediate.dense",
"encoder.encoder.layer.3.output.dense",
"encoder.encoder.layer.4.attention.attention.query",
"encoder.encoder.layer.4.attention.attention.key",
"encoder.encoder.layer.4.attention.attention.value",
"encoder.encoder.layer.4.attention.output.dense",
"encoder.encoder.layer.4.intermediate.dense",
"encoder.encoder.layer.4.output.dense",
"encoder.encoder.layer.5.attention.attention.query",
"encoder.encoder.layer.5.attention.attention.key",
"encoder.encoder.layer.5.attention.attention.value",
"encoder.encoder.layer.5.attention.output.dense",
"encoder.encoder.layer.5.intermediate.dense",
"encoder.encoder.layer.5.output.dense",
"encoder.encoder.layer.6.attention.attention.query",
"encoder.encoder.layer.6.attention.attention.key",
"encoder.encoder.layer.6.attention.attention.value",
"encoder.encoder.layer.6.attention.output.dense",
"encoder.encoder.layer.6.intermediate.dense",
"encoder.encoder.layer.6.output.dense",
"encoder.encoder.layer.7.attention.attention.query",
"encoder.encoder.layer.7.attention.attention.key",
"encoder.encoder.layer.7.attention.attention.value",
"encoder.encoder.layer.7.attention.output.dense",
"encoder.encoder.layer.7.intermediate.dense",
"encoder.encoder.layer.7.output.dense",
"encoder.encoder.layer.8.attention.attention.query",
"encoder.encoder.layer.8.attention.attention.key",
"encoder.encoder.layer.8.attention.attention.value",
"encoder.encoder.layer.8.attention.output.dense",
"encoder.encoder.layer.8.intermediate.dense",
"encoder.encoder.layer.8.output.dense",
"encoder.encoder.layer.9.attention.attention.query",
"encoder.encoder.layer.9.attention.attention.key",
"encoder.encoder.layer.9.attention.attention.value",
"encoder.encoder.layer.9.attention.output.dense",
"encoder.encoder.layer.9.intermediate.dense",
"encoder.encoder.layer.9.output.dense",
"encoder.encoder.layer.10.attention.attention.query",
"encoder.encoder.layer.10.attention.attention.key",
"encoder.encoder.layer.10.attention.attention.value",
"encoder.encoder.layer.10.attention.output.dense",
"encoder.encoder.layer.10.intermediate.dense",
"encoder.encoder.layer.10.output.dense",
"encoder.encoder.layer.11.attention.attention.query",
"encoder.encoder.layer.11.attention.attention.key",
"encoder.encoder.layer.11.attention.attention.value",
"encoder.encoder.layer.11.attention.output.dense",
"encoder.encoder.layer.11.intermediate.dense",
"encoder.encoder.layer.11.output.dense",
"encoder.pooler.dense",
"decoder.model.decoder.layers.0.self_attn.k_proj",
"decoder.model.decoder.layers.0.self_attn.v_proj",
"decoder.model.decoder.layers.0.self_attn.q_proj",
"decoder.model.decoder.layers.0.self_attn.out_proj",
"decoder.model.decoder.layers.0.encoder_attn.k_proj",
"decoder.model.decoder.layers.0.encoder_attn.v_proj",
"decoder.model.decoder.layers.0.encoder_attn.q_proj",
"decoder.model.decoder.layers.0.encoder_attn.out_proj",
"decoder.model.decoder.layers.0.fc1",
"decoder.model.decoder.layers.0.fc2",
"decoder.model.decoder.layers.1.self_attn.k_proj",
"decoder.model.decoder.layers.1.self_attn.v_proj",
"decoder.model.decoder.layers.1.self_attn.q_proj",
"decoder.model.decoder.layers.1.self_attn.out_proj",
"decoder.model.decoder.layers.1.encoder_attn.k_proj",
"decoder.model.decoder.layers.1.encoder_attn.v_proj",
"decoder.model.decoder.layers.1.encoder_attn.q_proj",
"decoder.model.decoder.layers.1.encoder_attn.out_proj",
"decoder.model.decoder.layers.1.fc1",
"decoder.model.decoder.layers.1.fc2",
"decoder.model.decoder.layers.2.self_attn.k_proj",
"decoder.model.decoder.layers.2.self_attn.v_proj",
"decoder.model.decoder.layers.2.self_attn.q_proj",
"decoder.model.decoder.layers.2.self_attn.out_proj",
"decoder.model.decoder.layers.2.encoder_attn.k_proj",
"decoder.model.decoder.layers.2.encoder_attn.v_proj",
"decoder.model.decoder.layers.2.encoder_attn.q_proj",
"decoder.model.decoder.layers.2.encoder_attn.out_proj",
"decoder.model.decoder.layers.2.fc1",
"decoder.model.decoder.layers.2.fc2",
"decoder.model.decoder.layers.3.self_attn.k_proj",
"decoder.model.decoder.layers.3.self_attn.v_proj",
"decoder.model.decoder.layers.3.self_attn.q_proj",
"decoder.model.decoder.layers.3.self_attn.out_proj",
"decoder.model.decoder.layers.3.encoder_attn.k_proj",
"decoder.model.decoder.layers.3.encoder_attn.v_proj",
"decoder.model.decoder.layers.3.encoder_attn.q_proj",
"decoder.model.decoder.layers.3.encoder_attn.out_proj",
"decoder.model.decoder.layers.3.fc1",
"decoder.model.decoder.layers.3.fc2",
"decoder.model.decoder.layers.4.self_attn.k_proj",
"decoder.model.decoder.layers.4.self_attn.v_proj",
"decoder.model.decoder.layers.4.self_attn.q_proj",
"decoder.model.decoder.layers.4.self_attn.out_proj",
"decoder.model.decoder.layers.4.encoder_attn.k_proj",
"decoder.model.decoder.layers.4.encoder_attn.v_proj",
"decoder.model.decoder.layers.4.encoder_attn.q_proj",
"decoder.model.decoder.layers.4.encoder_attn.out_proj",
"decoder.model.decoder.layers.4.fc1",
"decoder.model.decoder.layers.4.fc2",
"decoder.model.decoder.layers.5.self_attn.k_proj",
"decoder.model.decoder.layers.5.self_attn.v_proj",
"decoder.model.decoder.layers.5.self_attn.q_proj",
"decoder.model.decoder.layers.5.self_attn.out_proj",
"decoder.model.decoder.layers.5.encoder_attn.k_proj",
"decoder.model.decoder.layers.5.encoder_attn.v_proj",
"decoder.model.decoder.layers.5.encoder_attn.q_proj",
"decoder.model.decoder.layers.5.encoder_attn.out_proj",
"decoder.model.decoder.layers.5.fc1",
"decoder.model.decoder.layers.5.fc2",
"decoder.output_projection",
]

for i, j in loaded_state_dict.items():
    if isinstance(j, torch.Tensor):
        loaded_state_dict[i] = paddle.to_tensor(j.numpy())

# 使用t()转换权重矩阵
for name in convert_list:
    name2 = name + ".weight"
    if name2 in loaded_state_dict.keys():
        print(name2)
        loaded_state_dict[name2] = loaded_state_dict[name2].t()
        
paddle.save(loaded_state_dict, "./trocr-paddle/model_state.pdparams")

print("convert done")
